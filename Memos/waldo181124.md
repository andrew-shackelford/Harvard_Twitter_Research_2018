**Memo waldo181124.md**

*Extracting a subset of data from all the tweets*

The tweet files that we have been collecting contain a lot more information than we need for our work, so it makes
sense to extract a subset of the fields for our current work. This is true of all of the files, whether they were
collected because of the political hashtags (those with names beginning with *tag_data*), those collected because they 
explicitly mention a candidate (those with names starting with *candidate_mentions*) or those that contained the
name of a candidate somewhere in the tweet (those with names starting with *all_candidates*).

The program *reduce_tweets.py* does this for sets of files; while doing the reduction it also removes duplicate tweets
(that may have been collected for reasons of failure or because of mentioning multiple candidates) and will count the
number of tweets and the number of distinct tweeters. The program will also compress the tweet files to preserve
some disk space, as we were getting up to 14gigabytes of tweets on some days. 

The fields extracted from each tweet include (numbered by the index of the field once the line in the .csv is read):

    0. The id of the tweet, taken from the id_str field
    1. The id of the tweeter, taken from the id_str field of the user object
    2. A list of the hashtags associated with the tweet, taken from the hashtag field of the entities object. Only the
    text of the hashtag is extracted
    3. The geotag connected to the hashtag, if it exists; this is taken from the coordinates field
    4. The retweet status of the hashtag, taken from the retweeted_status field
    5. The retweet count of the hashtag, taken from the retweet_count field
    
For each entity that has made a tweet (at tweeter) we keep the following:

    0. The id of the tweeter, taken from the id_str
    1. The number of followers, taken from the followers_count
    2. The count of friends, taken from friends_count
    3. The number of tweets from this tweeter. 
    
The tweet extract is written on a per-file basis. The tweeter extraction is calculated over all of the file that are run
through the reduce_tweets.py program at the same time.

Note that the extracted tweets are written to a .csv file rather than a pickle file. This is because an earlier attempt 
to write a dictionary of extracted tweet fields seems to have broken the python pickle library, perhaps because it used
too much memory to store the dictionary. By writing to a .csv file, we can do line-at-a-time rather than building the
whole dicionary in memory. Given that the extracted .csv files run around 2 gigabytes each, this seems like a reasonable 
trade off. 

Trying to write the tweeter dictionary line-at-a-time did not seem to help the runtime at all, so I kept with the pickle
of the dictionary. This is true even though the tweeter dictionary is built for the entire set of files, while the tweet
reduction is done on a file-by-file basis. 

The program is also built to un-compress or un-zip the .json files if needed, and then to gzip then when they are finished
being used. I moved from **compresss** to **gzip** when I discovered that the files compressed by the latter were 
considerably smaller than those compressed by the former. 

If we decide that we want to extract other fields from the tweets, we can do so by simply changing the tweet_tag or
tweeter classes, found in *tweet_reduction_cl.py*. Re-running the reducer will take about 48 hours on my desktop machine 
(which has the advantage of having 64 gig of main memory, and so never found that it had memory pressure from structures
that were too large). 